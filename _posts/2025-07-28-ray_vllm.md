---
layout: post
title: "Exploring the Integration of the Latest Version of vLLM with Ray 2.35.0: Gaps, Issues, and Challenges"
date:   2025-07-29
tags: [AI Infra, åˆ†å¸ƒå¼]
comments: true
author: marshall
---

æœ€è¿‘å¤§åŠå¹´éƒ½åœ¨åšAI Infraç›¸å…³çš„å­¦ä¹ ï¼Œä¹Ÿç®—æ˜¯å°æœ‰æ”¶è·ï¼Œå‚ä¸äº†Ray/vLLMç¤¾åŒºï¼Œæäº†ä¸€äº›issueå’ŒprğŸ˜Šï¼Œè¿™ä¸ªé¢†åŸŸå¾ˆåˆæˆ‘èƒƒå£ï¼Œåç»­å¯èƒ½ä¹Ÿä¼šfocusè¿™è¾¹ç»§ç»­æ·±è€•ã€‚

èƒŒæ™¯ï¼šå…¬å¸å†…éƒ¨ç”¨çš„rayæ˜¯é­”æ”¹çš„ray2.35.0ï¼Œè€Œä¸šåŠ¡éƒ¨é—¨æœ‰éœ€æ±‚å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬çš„vllmï¼Œå®˜æ–¹çš„setupå·²ç»è¦æ±‚ä¾èµ–çš„rayçš„ç‰ˆæœ¬å¤§äºç­‰äº2.46ï¼Œå› æ­¤æœ‰å¿…è¦æµ‹è¯•ä¸€ä¸‹è¿™ä¹‹é—´æœ‰å¤šå°‘Gapã€‚é€šè¿‡è¿™ç¯‡æ–‡ç« ï¼ˆæ›´åƒæ˜¯å®éªŒè®°å½•ï¼‰ï¼Œä½ å¯ä»¥å¿«é€Ÿäº†è§£åˆ°Rayä¸vLLMçš„è€¦åˆç¨‹åº¦ï¼Œæ–‡ä¸­ä¹Ÿç»™å‡ºäº†å¾ˆå¤šä»£ç æˆ–é“¾æ¥æä¾›å¿«é€ŸéªŒè¯ã€‚

å£°æ˜ï¼šæœ¬æ–‡æ¶‰åŠçš„å†…å®¹å·²ç»è¿‡ç­›é€‰ï¼Œç¡®ä¿å…¨éƒ¨æ¥æºäºå¼€æºç¤¾åŒºã€‚å¦å¤–ç”±äºæ˜¯ç›´æ¥æ¬è¿ä¸ªäººç»™å…¬å¸å†…éƒ¨å†™çš„reportï¼Œå› æ­¤è¯­è¨€æ˜¯è‹±æ–‡ï¼Œè§è°…ã€‚

<!-- more -->
<!-- meta name="description" -->

## Main source codes

vLLM set Ray as default multi-node executor backend, the main logic are as follows:

[https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_distributed_executor.py](https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_distributed_executor.py)
[https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_utils.py](https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_utils.py)

Below are the core features of Ray has been used by vLLM:
- Ray Actors / Workers
- Ray DAG (Compiled Graphs)
- Ray Cluster (Placement Groups, Resourcesâ€¦)

## Use Case Limitations
### 1. Ray Data LLM API

[https://docs.ray.io/en/latest/data/working-with-llms.html#vllm-llm](https://docs.ray.io/en/latest/data/working-with-llms.html#vllm-llm)
[https://docs.vllm.ai/en/latest/serving/offline_inference.html?h=ray#ray-data-llm-api](https://docs.vllm.ai/en/latest/serving/offline_inference.html?h=ray#ray-data-llm-api)

```py
import ray  # Requires ray>=2.44.1
from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor

config = vLLMEngineProcessorConfig(model_source="unsloth/Llama-3.2-1B-Instruct")
processor = build_llm_processor(
    config,
    preprocess=lambda row: {
        "messages": [
            {"role": "system", "content": "You are a bot that completes unfinished haikus."},
            {"role": "user", "content": row["item"]},
        ],
        "sampling_params": {"temperature": 0.3, "max_tokens": 250},
    },
    postprocess=lambda row: {"answer": row["generated_text"]},
)

ds = ray.data.from_items(["An old silent pond..."])
ds = processor(ds)
ds.write_parquet("local:///tmp/data/")
```
### 2. Batch LLM Inference (also due to lack of ray data LLM api)
[https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference.html?h=ray](https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference.html?h=ray)

### 3. Ray Serve LLM API
[https://docs.ray.io/en/latest/ray-overview/examples/e2e-rag/notebooks/03_Deploy_LLM_with_Ray_Serve.html#deploy-llm-with-ray-serve-llm](https://docs.ray.io/en/latest/ray-overview/examples/e2e-rag/notebooks/03_Deploy_LLM_with_Ray_Serve.html#deploy-llm-with-ray-serve-llm)

```py
from ray import serve
from ray.serve.llm import LLMConfig


llm_config = LLMConfig(
    model_loading_config={
        'model_id': 'Qwen/Qwen2.5-32B-Instruct'
    },
    engine_kwargs={
        'max_num_batched_tokens': 8192,
        'max_model_len': 8192,
        'max_num_seqs': 64,
        'tensor_parallel_size': 4,
        'trust_remote_code': True,
    },
    accelerator_type='L4',
    deployment_config={
        'autoscaling_config': {
            'target_ongoing_requests': 32,
            'target_num_ongoing_requests_per_replica': 32,
        },
        'max_ongoing_requests': 64,
    },
)
```

### 4. Ray Compile graph
Ray version required 2.43.0

[https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_distributed_executor.py#L517](https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_distributed_executor.py#L517)

```py
required_version = version.parse("2.43.0")
current_version = version.parse(importlib.metadata.version("ray"))
if current_version < required_version:
   raise ValueError(f"Ray version {required_version} is "
                    f"required, but found {current_version}")


import importlib.util
cgraph_spec = importlib.util.find_spec(
   "ray.experimental.compiled_dag_ref")
```

which errors will occur:

```bash
ERROR 07-31 04:43:09 [engine.py:165] TypeError: DAGNode.experimental_compile() got an unexpected keyword argument '_overlap_gpu_communication'
```

Related vllm PR: [https://github.com/vllm-project/vllm/pull/13994](https://github.com/vllm-project/vllm/pull/13994)

Related ray feature: [https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html](https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html)
[https://docs.ray.io/en/latest/ray-core/compiled-graph/overlap.html#experimental-overlapping-communication-and-computation](https://docs.ray.io/en/latest/ray-core/compiled-graph/overlap.html#experimental-overlapping-communication-and-computation)

It is a gap yet, we could set `VLLM_USE_RAY_COMPILED_DAG=0` to avoid this feture

## Tested Cases (Passed)

Basic use with ray job  (tensor_parallel_size > 1)
```py
from vllm import LLM
llm = LLM("facebook/opt-125m", tensor_parallel_size=2)
output = llm.generate("San Franciso is a")
```

Vllm serve using ray
```bash
python3 -m vllm.entrypoints.openai.api_server --port 8080 --model Qwen/Qwen2.5-1.5B-Instruct --tensor-parallel-size 2 --distributed-executor-backend=ray
```

Quick start:
[https://docs.vllm.ai/en/latest/getting_started/quickstart.html](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)

Multi-Node Serving: 
[https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving.html](https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving.html)

RLHF using ray:
[https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf.html?h=rlhf](https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf.html?h=rlhf)


## Related Discussion

- [https://github.com/vllm-project/vllm/issues/20860](https://github.com/vllm-project/vllm/issues/20860)
- [https://github.com/vllm-project/vllm/issues/20476](https://github.com/vllm-project/vllm/issues/20476)

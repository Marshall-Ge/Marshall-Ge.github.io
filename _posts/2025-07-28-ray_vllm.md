---
layout: post
title: "Exploring the Integration of the Latest Version of vLLM with Ray 2.35.0: Gaps, Issues, and Challenges"
date:   2025-07-29
tags: [AI Infra, åˆ†å¸ƒå¼]
comments: true
author: marshall
---

æœ€è¿‘å¤§åŠå¹´éƒ½åœ¨åšAI Infraç›¸å…³çš„å­¦ä¹ ï¼Œä¹Ÿç®—æ˜¯å°æœ‰æ”¶è·ï¼Œå‚ä¸äº†Ray/vLLMç¤¾åŒºï¼Œæäº†ä¸€äº›issueå’ŒprğŸ˜Šï¼Œè¿™ä¸ªé¢†åŸŸå¾ˆåˆæˆ‘èƒƒå£ï¼Œåç»­å¯èƒ½ä¹Ÿä¼šfocusè¿™è¾¹ç»§ç»­æ·±è€•ã€‚

èƒŒæ™¯ï¼šå…¬å¸å†…éƒ¨ç”¨çš„rayæ˜¯é­”æ”¹çš„ray2.35.0ï¼Œè€Œä¸šåŠ¡éƒ¨é—¨æœ‰éœ€æ±‚å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬çš„vllmï¼Œå®˜æ–¹çš„setupå·²ç»è¦æ±‚ä¾èµ–çš„rayçš„ç‰ˆæœ¬å¤§äºç­‰äº2.46ï¼Œå› æ­¤æœ‰å¿…è¦æµ‹è¯•ä¸€ä¸‹è¿™ä¹‹é—´æœ‰å¤šå°‘Gapã€‚é€šè¿‡è¿™ç¯‡æ–‡ç« ï¼ˆæ›´åƒæ˜¯å®éªŒè®°å½•ï¼‰ï¼Œä½ å¯ä»¥å¿«é€Ÿäº†è§£åˆ°Rayä¸vLLMçš„è€¦åˆç¨‹åº¦ï¼Œæ–‡ä¸­ä¹Ÿç»™å‡ºäº†å¾ˆå¤šä»£ç æˆ–é“¾æ¥æä¾›å¿«é€ŸéªŒè¯ã€‚

å£°æ˜ï¼šæœ¬æ–‡æ¶‰åŠçš„å†…å®¹å·²ç»è¿‡ç­›é€‰ï¼Œç¡®ä¿å…¨éƒ¨æ¥æºäºå¼€æºç¤¾åŒºã€‚å¦å¤–ç”±äºæ˜¯ç›´æ¥æ¬è¿ä¸ªäººç»™å…¬å¸å†…éƒ¨å†™çš„reportï¼Œå› æ­¤è¯­è¨€æ˜¯è‹±æ–‡ï¼Œè§è°…ã€‚

<!-- more -->
<!-- meta name="description" -->

## Main source codes

vLLM set Ray as default multi-node executor backend, the main logic are as follows:

https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_distributed_executor.py
https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_utils.py

Below are the core features of Ray has been used by vLLM:
- Ray Actors / Workers
- Ray DAG (Compiled Graphs)
- Ray Cluster (Placement Groups, Resourcesâ€¦)

## Use Case Limitations

Ray Data LLM API

https://docs.ray.io/en/latest/data/working-with-llms.html#vllm-llm
https://docs.vllm.ai/en/latest/serving/offline_inference.html?h=ray#ray-data-llm-api

```{python}
import ray  # Requires ray>=2.44.1
from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor

config = vLLMEngineProcessorConfig(model_source="unsloth/Llama-3.2-1B-Instruct")
processor = build_llm_processor(
    config,
    preprocess=lambda row: {
        "messages": [
            {"role": "system", "content": "You are a bot that completes unfinished haikus."},
            {"role": "user", "content": row["item"]},
        ],
        "sampling_params": {"temperature": 0.3, "max_tokens": 250},
    },
    postprocess=lambda row: {"answer": row["generated_text"]},
)

ds = ray.data.from_items(["An old silent pond..."])
ds = processor(ds)
ds.write_parquet("local:///tmp/data/")
```

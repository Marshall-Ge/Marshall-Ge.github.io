---
layout: post
title: "Exploring the Integration of the Latest Version of vLLM with Ray 2.35.0: Gaps, Issues, and Challenges"
date:   2025-07-29
tags: [AI Infra, 分布式]
comments: true
author: marshall
---

最近大半年都在做AI Infra相关的学习，也算是小有收获，参与了Ray/vLLM社区，提了一些issue和pr😊，这个领域很合我胃口，后续可能也会focus这边继续深耕。

背景：公司内部用的ray是魔改的ray2.35.0，而业务部门有需求升级到最新版本的vllm，官方的setup已经要求依赖的ray的版本大于等于2.46，因此有必要测试一下这之间有多少Gap。通过这篇文章（更像是实验记录），你可以快速了解到Ray与vLLM的耦合程度，文中也给出了很多代码或链接提供快速验证。

声明：本文涉及的内容已经过筛选，确保全部来源于开源社区。另外由于是直接搬运个人给公司内部写的report，因此语言是英文，见谅。

<!-- more -->
<!-- meta name="description" -->

## Main source codes

vLLM set Ray as default multi-node executor backend, the main logic are as follows:

https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_distributed_executor.py
https://github.com/vllm-project/vllm/blob/main/vllm/executor/ray_utils.py

Below are the core features of Ray has been used by vLLM:
- Ray Actors / Workers
- Ray DAG (Compiled Graphs)
- Ray Cluster (Placement Groups, Resources…)

## Use Case Limitations

Ray Data LLM API

https://docs.ray.io/en/latest/data/working-with-llms.html#vllm-llm
https://docs.vllm.ai/en/latest/serving/offline_inference.html?h=ray#ray-data-llm-api

```{python}
import ray  # Requires ray>=2.44.1
from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor

config = vLLMEngineProcessorConfig(model_source="unsloth/Llama-3.2-1B-Instruct")
processor = build_llm_processor(
    config,
    preprocess=lambda row: {
        "messages": [
            {"role": "system", "content": "You are a bot that completes unfinished haikus."},
            {"role": "user", "content": row["item"]},
        ],
        "sampling_params": {"temperature": 0.3, "max_tokens": 250},
    },
    postprocess=lambda row: {"answer": row["generated_text"]},
)

ds = ray.data.from_items(["An old silent pond..."])
ds = processor(ds)
ds.write_parquet("local:///tmp/data/")
```
